{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMf221965OW5"
   },
   "source": [
    "# **GOAL**: To Build a Sarcasm Classfier using ML and NLP \n",
    "**Problem description**:-\n",
    "We have given the News Headlines Dataset and our goal is to predict whether a given text is sarcastic or not.\n",
    "\n",
    "**Dataset**:-This News Headlines Dataset for Sarcasm Detection is collected from The Onion website which aims at producing sarcastic versions of current events and HuffPost website which collects real news headlines.\n",
    "- Download the dataset [here.](https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection)\n",
    "\n",
    "Useful Blogs:\n",
    "- https://monkeylearn.com/blog/text-cleaning/\n",
    "- https://www.geeksforgeeks.org/python-stemming-words-with-nltk/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3vpNQp2RPkkB",
    "outputId": "85c096f7-12a2-454d-cb5f-a6af9b7d57f5"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNflJxxTmTSo",
    "outputId": "6fe7abf8-6952-4195-cb51-f9902526b21c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyforest in /Users/rahulmullaguru/opt/anaconda3/lib/python3.8/site-packages (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyforest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PXyWUdK_q-5"
   },
   "source": [
    "**Intro about NLTK(Natural Language Toolkit)**:\n",
    "\n",
    "NLTK is a platform used for building Python programs that work with human language data for applying in statistical natural language processing (NLP). It contains text processing libraries for tokenization, parsing, classification, stemming, tagging and semantic reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "d1fJEd2LbGu4",
    "outputId": "48a8980f-256e-410b-ea02-c9d9bc29eb74"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rahulmullaguru/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/rahulmullaguru/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rahulmullaguru/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pyforest #to import all the python libraries at once\n",
    "import json\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"wordnet\")\n",
    "import nltk.corpus\n",
    "from nltk import punkt\n",
    "from nltk import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "tEwhiQxwTYKs",
    "outputId": "3c7669f1-2fa5-498f-86d0-7a2d3db56389"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import nltk\\nimport pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#to read the data in json file and creating a d'frame out of it\n",
    "data=pd.read_json(\"Sarcasm_Headlines_Dataset.json\", lines=True)\n",
    "\n",
    "# For Rahul Narang-- Drive location\n",
    "#data= pd.read_json(\"/content/drive/MyDrive/Sarcasm_Headlines_Dataset.json\", lines=True)\n",
    "#data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t6AmNVe160Hz",
    "outputId": "f38e7050-91f2-4cd3-8c32-a96a6fc160e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_link    False\n",
      "headline        False\n",
      "is_sarcastic    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().any(axis=0)) #check for any null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "id": "3qmDoQrV7MpI",
    "outputId": "565dede4-a92b-4e4c-f94d-a15abc3153eb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret  b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the  roseanne  revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son s web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen  not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j k  rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/advancing...</td>\n",
       "      <td>advancing the world s women</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/how-meat-...</td>\n",
       "      <td>the fascinating case for eating lab grown meat</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/boxed-col...</td>\n",
       "      <td>this ceo will send your kids to school  if you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://politics.theonion.com/top-snake-handle...</td>\n",
       "      <td>top snake handler leaves sinking huckabee camp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/fridays-m...</td>\n",
       "      <td>friday s morning email  inside trump s presser...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/airline-p...</td>\n",
       "      <td>airline passengers tackle man who rushes cockp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/facebook-...</td>\n",
       "      <td>facebook reportedly working on healthcare feat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://www.huffingtonpost.comhttp://www.thegu...</td>\n",
       "      <td>north korea praises trump and urges us voters ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jeffrey-l...</td>\n",
       "      <td>actually  cnn s jeffrey lord has been  indefen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/barcelona...</td>\n",
       "      <td>barcelona holds huge protest in support of ref...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://entertainment.theonion.com/nuclear-bom...</td>\n",
       "      <td>nuclear bomb detonates during rehearsal for  s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://www.theonion.com/cosby-lawyer-asks-why...</td>\n",
       "      <td>cosby lawyer asks why accusers didn t come for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://www.theonion.com/stock-analysts-confus...</td>\n",
       "      <td>stock analysts confused  frightened by boar ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/bloomberg...</td>\n",
       "      <td>bloomberg s program to build better cities jus...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/craig-hic...</td>\n",
       "      <td>craig hicks indicted</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         article_link  \\\n",
       "0   https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1   https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2   https://local.theonion.com/mom-starting-to-fea...   \n",
       "3   https://politics.theonion.com/boehner-just-wan...   \n",
       "4   https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "5   https://www.huffingtonpost.com/entry/advancing...   \n",
       "6   https://www.huffingtonpost.com/entry/how-meat-...   \n",
       "7   https://www.huffingtonpost.com/entry/boxed-col...   \n",
       "8   https://politics.theonion.com/top-snake-handle...   \n",
       "9   https://www.huffingtonpost.com/entry/fridays-m...   \n",
       "10  https://www.huffingtonpost.com/entry/airline-p...   \n",
       "11  https://www.huffingtonpost.com/entry/facebook-...   \n",
       "12  https://www.huffingtonpost.comhttp://www.thegu...   \n",
       "13  https://www.huffingtonpost.com/entry/jeffrey-l...   \n",
       "14  https://www.huffingtonpost.com/entry/barcelona...   \n",
       "15  https://entertainment.theonion.com/nuclear-bom...   \n",
       "16  https://www.theonion.com/cosby-lawyer-asks-why...   \n",
       "17  https://www.theonion.com/stock-analysts-confus...   \n",
       "18  https://www.huffingtonpost.com/entry/bloomberg...   \n",
       "19  https://www.huffingtonpost.com/entry/craig-hic...   \n",
       "\n",
       "                                             headline  is_sarcastic  \n",
       "0   former versace store clerk sues over secret  b...             0  \n",
       "1   the  roseanne  revival catches up to our thorn...             0  \n",
       "2   mom starting to fear son s web series closest ...             1  \n",
       "3   boehner just wants wife to listen  not come up...             1  \n",
       "4   j k  rowling wishes snape happy birthday in th...             0  \n",
       "5                         advancing the world s women             0  \n",
       "6      the fascinating case for eating lab grown meat             0  \n",
       "7   this ceo will send your kids to school  if you...             0  \n",
       "8   top snake handler leaves sinking huckabee camp...             1  \n",
       "9   friday s morning email  inside trump s presser...             0  \n",
       "10  airline passengers tackle man who rushes cockp...             0  \n",
       "11  facebook reportedly working on healthcare feat...             0  \n",
       "12  north korea praises trump and urges us voters ...             0  \n",
       "13  actually  cnn s jeffrey lord has been  indefen...             0  \n",
       "14  barcelona holds huge protest in support of ref...             0  \n",
       "15  nuclear bomb detonates during rehearsal for  s...             1  \n",
       "16  cosby lawyer asks why accusers didn t come for...             1  \n",
       "17  stock analysts confused  frightened by boar ma...             1  \n",
       "18  bloomberg s program to build better cities jus...             0  \n",
       "19                               craig hicks indicted             0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "#as we observed some special symbols in the text under \"headline\"., we removed them using library \"re\"\n",
    "data['headline'] = data['headline'].map(lambda x: re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", ' ', x))\n",
    "data.head(20)\n",
    "#/^[a-zA-Z0-9!@#$%^&*()_+\\-=\\[\\]{};':\"\\\\|,.<>\\/?]*$/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "jPRnVM23mXdG",
    "outputId": "02f4dd25-8646-4848-8888-ab5bdc3826dc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>text</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret  b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the  roseanne  revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son s web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen  not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j k  rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26704</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/american-...</td>\n",
       "      <td>american politics in moral free fall</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26705</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/americas-...</td>\n",
       "      <td>america s best 20 hikes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26706</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/reparatio...</td>\n",
       "      <td>reparations and obama</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26707</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/israeli-b...</td>\n",
       "      <td>israeli ban targeting boycott supporters raise...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26708</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/gourmet-g...</td>\n",
       "      <td>gourmet gifts for the foodie 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26709 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            article_link  \\\n",
       "0      https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1      https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2      https://local.theonion.com/mom-starting-to-fea...   \n",
       "3      https://politics.theonion.com/boehner-just-wan...   \n",
       "4      https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "...                                                  ...   \n",
       "26704  https://www.huffingtonpost.com/entry/american-...   \n",
       "26705  https://www.huffingtonpost.com/entry/americas-...   \n",
       "26706  https://www.huffingtonpost.com/entry/reparatio...   \n",
       "26707  https://www.huffingtonpost.com/entry/israeli-b...   \n",
       "26708  https://www.huffingtonpost.com/entry/gourmet-g...   \n",
       "\n",
       "                                                    text  is_sarcastic  \n",
       "0      former versace store clerk sues over secret  b...             0  \n",
       "1      the  roseanne  revival catches up to our thorn...             0  \n",
       "2      mom starting to fear son s web series closest ...             1  \n",
       "3      boehner just wants wife to listen  not come up...             1  \n",
       "4      j k  rowling wishes snape happy birthday in th...             0  \n",
       "...                                                  ...           ...  \n",
       "26704               american politics in moral free fall             0  \n",
       "26705                            america s best 20 hikes             0  \n",
       "26706                              reparations and obama             0  \n",
       "26707  israeli ban targeting boycott supporters raise...             0  \n",
       "26708                  gourmet gifts for the foodie 2014             0  \n",
       "\n",
       "[26709 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#renaming the \"headline\" column as \"text\"\n",
    "data=data.rename(columns={\"headline\":\"text\"})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "R9TMgq4SnFVp"
   },
   "outputs": [],
   "source": [
    "text = data[\"text\"]\n",
    "is_sarcastic = data[\"is_sarcastic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HNX1WzI27iyJ",
    "outputId": "d58b2eed-7ad4-4c7e-e466-f792925332e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        former versace store clerk sues over secret  b...\n",
       "1        the  roseanne  revival catches up to our thorn...\n",
       "2        mom starting to fear son s web series closest ...\n",
       "3        boehner just wants wife to listen  not come up...\n",
       "4        j k  rowling wishes snape happy birthday in th...\n",
       "                               ...                        \n",
       "26704                 american politics in moral free fall\n",
       "26705                              america s best 20 hikes\n",
       "26706                                reparations and obama\n",
       "26707    israeli ban targeting boycott supporters raise...\n",
       "26708                    gourmet gifts for the foodie 2014\n",
       "Name: text, Length: 26709, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.apply(lambda x: x.lower()) #axis=1 applies to each row\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "iwagmHOOBeIV"
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "text = text.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "r73KU4JmnErL",
    "outputId": "a486d0db-9d28-4d5d-ef2b-a7fc00490028"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26709, 3)\n",
      "0    14985\n",
      "1    11724\n",
      "Name: is_sarcastic, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>text</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret  b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the  roseanne  revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j k  rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/advancing...</td>\n",
       "      <td>advancing the world s women</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/how-meat-...</td>\n",
       "      <td>the fascinating case for eating lab grown meat</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "5  https://www.huffingtonpost.com/entry/advancing...   \n",
       "6  https://www.huffingtonpost.com/entry/how-meat-...   \n",
       "\n",
       "                                                text  is_sarcastic  \n",
       "0  former versace store clerk sues over secret  b...             0  \n",
       "1  the  roseanne  revival catches up to our thorn...             0  \n",
       "4  j k  rowling wishes snape happy birthday in th...             0  \n",
       "5                        advancing the world s women             0  \n",
       "6     the fascinating case for eating lab grown meat             0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data['is_sarcastic'].value_counts())\n",
    "data[data[\"is_sarcastic\"]==0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWlDOoOB5Spw"
   },
   "source": [
    "Preprocessing Steps:\n",
    "1. Stemming\n",
    "2. Vectorization \n",
    "3. Tokenization\n",
    "4. Lemmatization\n",
    "\n",
    "## Stemming \n",
    "Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or the roots of words known as a lemma. Stemming is important in natural language understanding (NLU) and natural language processing (NLP).\n",
    "for example:-\n",
    "original word: “reading” and “reader”.\n",
    "after stemming: read (for all the above two words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "r5y3KrjlnFmN"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "ps = PorterStemmer()\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "hwZfRhYiLWY6"
   },
   "outputs": [],
   "source": [
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(ps.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "DPG1XSeOR_Sv"
   },
   "outputs": [],
   "source": [
    "textstem = text.apply(lambda x: stemSentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iXnpwMPiSM7L",
    "outputId": "6c342eeb-416e-4b7c-cbf0-46061f679ba0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        former versac store clerk sue secret black cod...\n",
       "1        roseann reviv catch thorni polit mood better w...\n",
       "2        mom start fear son web seri closest thing gran...\n",
       "3        boehner want wife listen come altern debt redu...\n",
       "4            j k rowl wish snape happi birthday magic way \n",
       "                               ...                        \n",
       "26704                      american polit moral free fall \n",
       "26705                                america best 20 hike \n",
       "26706                                         repar obama \n",
       "26707    isra ban target boycott support rais alarm abr...\n",
       "26708                             gourmet gift foodi 2014 \n",
       "Name: text, Length: 26709, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textstem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPLanMLMQhD_"
   },
   "source": [
    "```\n",
    "#alternative\n",
    "text = text.apply(lambda x: x.split())\n",
    "text = text.apply(lambda x :\" \".join([ps.stem(word) for word in x]))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ck-Axdg-Uq6Y"
   },
   "source": [
    "## Lemmatization  \n",
    "Lemmatization, on the other hand, groups words based on root definition, and allows us to differentiate between present, past, and indefinite.\n",
    "So, ‘jumps’ and ‘jump’ are grouped into the present ‘jump’, as different from all uses of ‘jumped’ which are grouped together as past tense, and all instances of ‘jumping’ which are grouped together as the indefinite (meaning continuing/continuous).\n",
    "\n",
    "**Note:** \n",
    "In general, lemmatization offers better precision than stemming, but at the expense of recall. As we've seen, stemming and lemmatization are effective techniques to expand recall, with lemmatization giving up some of that recall to increase precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "xN5V1UlqnFyc"
   },
   "outputs": [],
   "source": [
    "lm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "x-duz0HnnF3O"
   },
   "outputs": [],
   "source": [
    "def LemmSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    lemm_sentence=[]\n",
    "    for word in token_words:\n",
    "        lemm_sentence.append(lm.lemmatize(word))\n",
    "        lemm_sentence.append(\" \")\n",
    "    return \"\".join(lemm_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDoCNjJrnF8I",
    "outputId": "2e9dd468-8841-42e5-ebf1-dedba54b1830"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        former versace store clerk sue secret black co...\n",
       "1        roseanne revival catch thorny political mood b...\n",
       "2        mom starting fear son web series closest thing...\n",
       "3        boehner want wife listen come alternative debt...\n",
       "4        j k rowling wish snape happy birthday magical ...\n",
       "                               ...                        \n",
       "26704                   american politics moral free fall \n",
       "26705                                america best 20 hike \n",
       "26706                                    reparation obama \n",
       "26707    israeli ban targeting boycott supporter raise ...\n",
       "26708                            gourmet gift foodie 2014 \n",
       "Name: text, Length: 26709, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textlemm = text.apply(lambda x: LemmSentence(x))\n",
    "textlemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "4MGQY3fQgVIe"
   },
   "outputs": [],
   "source": [
    "data['textlemm'] = textlemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DouwVQNOUBX3",
    "outputId": "74b7d8b8-9e49-4d13-cc00-63a3e2e87506"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    former versac store clerk sue secret black cod...\n",
       "1    roseann reviv catch thorni polit mood better w...\n",
       "2    mom start fear son web seri closest thing gran...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textstem [:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Pwt3aNJJy9c"
   },
   "source": [
    "## Vectorization:\n",
    "**Some popular methods to accomplish text vectorization:**\n",
    "\n",
    "* Binary Term Frequency.\n",
    "* Bag of Words (BoW) Term Frequency.\n",
    "* (L1) Normalized Term Frequency.\n",
    "* (L2) Normalized TF-IDF.\n",
    "* Word2Vec.\n",
    "\n",
    "Corpus - Collection of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "dSQWqQHjUHxv"
   },
   "outputs": [],
   "source": [
    "# Text Vectorization\n",
    "# Text Vectorization is the process of converting text into numerical representation\n",
    "# In the NLP world, it is known as Word embeddings.\n",
    "#Text Vectorization\n",
    "# Text Vectorization is the process of converting text into numerical representation\n",
    "# In the NLP world, it is known as word embeddings.\n",
    "##Link: https://www.analyticsvidhya.com/blog/2021/06/part-5-step-by-step-guide-to-master-nlp-text-vectorization-approaches/\n",
    "#Term frequency-inverse document frequency ( TF-IDF) \n",
    "#gives a measure that takes the importance of a word\n",
    "#into consideration depending on how frequently it occurs in a document and a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "C7nvpJg7UIEX"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer(max_features = 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "_eFTOzPfUIS3"
   },
   "outputs": [],
   "source": [
    "text_tfidf = list(textlemm)\n",
    "text_tfidf = tv.fit_transform(textlemm).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "zrbWTXNVFSbA"
   },
   "outputs": [],
   "source": [
    "#text_1 = list(text)\n",
    "#text_1 = tv.fit_transform(text).toarray()\n",
    "#print(text_1)\n",
    "#text_2= tv.fit_transform(text)\n",
    "#feature_names= tv.get_feature_names()\n",
    "#print(text_2)\n",
    "\n",
    "#feature_names= tv.get_feature_names()\n",
    "#feature_names1= tv.get_feature_names()\n",
    "# 0: 394 are the numerical values \n",
    "# we need text only as part of NLP\n",
    "#print(feature_names1[394:500])\n",
    "\n",
    "#matrix= vectors.todense()\n",
    "#list_dense = matrix.tolist()\n",
    "#df1= pd.DataFrame(text_1)\n",
    "#print(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3q_w-8VDl7FL"
   },
   "source": [
    "Params -\n",
    "* size: The number of dimensions of the embeddings and the default is 100.\n",
    "* window: The maximum distance between a target word and words around the target word. The default window is 5.\n",
    "* min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5.\n",
    "* workers: The number of partitions during training and the default workers is 3.\n",
    "* sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "lnIF2CskKIiu"
   },
   "outputs": [],
   "source": [
    "features=text_tfidf\n",
    "labels = is_sarcastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "f_XA8LkDKRlX"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.1, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vM8jwF85Ll4a",
    "outputId": "1a33b08f-d9b8-437f-bf2d-83e6f6a836a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13831    1\n",
       " 18506    1\n",
       " 24970    0\n",
       " 19892    0\n",
       " 2733     0\n",
       "         ..\n",
       " 13123    0\n",
       " 19648    0\n",
       " 9845     1\n",
       " 10799    0\n",
       " 2732     0\n",
       " Name: is_sarcastic, Length: 24038, dtype: int64,\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train, features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "VqC2NvjGyylS"
   },
   "outputs": [],
   "source": [
    "##Choosing the Right Alogorthm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "IcXfJW539bS6"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B7ZH6Dlr7_rC",
    "outputId": "488d3d09-5dbe-40e0-f161-6e4c382a0bdf"
   },
   "outputs": [],
   "source": [
    "#lsvc = LinearSVC()\n",
    "# training the model\n",
    "#lsvc.fit(features_train, labels_train)\n",
    "# getting the score of train and test data\n",
    "#print(lsvc.score(features_train, labels_train)) # 90.93\n",
    "#print(lsvc.score(features_test, labels_test))   # 83.75\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "NdoW8hsI7_nM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9128463266494716\n",
      "0.7147135904155747\n"
     ]
    }
   ],
   "source": [
    "# model 2:-\n",
    "# Using Gaussuan Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(features_train, labels_train)\n",
    "print(gnb.score(features_train, labels_train))  # 78.86\n",
    "print(gnb.score(features_test, labels_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kuTkmzav7_ix"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6CvCAjzK7_ek"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5zXQNBJl7_WV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dR03dkbZ7_S6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j56yF49E7_O7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFGxRJ397_IM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEPbAHwWUIfT"
   },
   "source": [
    "# We will use Word2Vec to get vectors which represents words in Multi-dimensional space (ex: 30d or 32d)\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMghnBdmnndc"
   },
   "outputs": [],
   "source": [
    "data['w2v']= data['textlemm'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9yFl3LbsbNVj"
   },
   "outputs": [],
   "source": [
    "Word2Vec_1 = Word2Vec(data['w2v'], \n",
    "                        min_count= 2,\n",
    "                        window=4,\n",
    "                        size=500,\n",
    "                        alpha=0.03, \n",
    "                        min_alpha=0.0007, \n",
    "                        sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aor9vg1jlWWx"
   },
   "outputs": [],
   "source": [
    "Word2Vec_1.wv.vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DTG6Gfa3Vo2o"
   },
   "outputs": [],
   "source": [
    "Word2Vec_1 = Word2Vec(data['w2v'], \n",
    "                        min_count= 2,\n",
    "                        window=4,\n",
    "                        size=300, \n",
    "                        alpha=0.03, \n",
    "                        min_alpha=0.0007, \n",
    "                        sg = 1)\n",
    "w2v_model.build_vocab(w2v_df, progress_per=10000)\n",
    "w2v_model.train(w2v_df, total_examples=w2v_model.corpus_count, epochs=100, report_delay=1)\n",
    "    return w2v_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37LKVBLsbfrQ"
   },
   "outputs": [],
   "source": [
    "Word2Vec_1.wv.vector_size\n",
    "\n",
    "# Word2Vec_1.vector_size -- also works the same way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLCQ7CNNekqv"
   },
   "source": [
    "# Another method of dealing with the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Po_fh9bZs7S"
   },
   "outputs": [],
   "source": [
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json \\\n",
    "    -O /tmp/sarcasm.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PsvJqgONdpCC"
   },
   "outputs": [],
   "source": [
    "with open(\"/tmp/sarcasm.json\",\"r\") as f:\n",
    "  datastore=json.load(f)\n",
    "sentences = []\n",
    "labels=[]\n",
    "url[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I8oIb06qdt1U",
    "outputId": "929c289a-aeab-4a13-dbba-87f401c09d77"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'article_link': 'https://www.huffingtonpost.com/entry/roseanne-revival-review_us_5ab3a497e4b054d118e04365',\n",
       "  'headline': \"the 'roseanne' revival catches up to our thorny political mood, for better and worse\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/mom-starting-to-fear-son-s-web-series-closest-thing-she-1819576697',\n",
       "  'headline': \"mom starting to fear son's web series closest thing she will have to grandchild\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/boehner-just-wants-wife-to-listen-not-come-up-with-alt-1819574302',\n",
       "  'headline': 'boehner just wants wife to listen, not come up with alternative debt-reduction ideas',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/jk-rowling-wishes-snape-happy-birthday_us_569117c4e4b0cad15e64fdcb',\n",
       "  'headline': 'j.k. rowling wishes snape happy birthday in the most magical way',\n",
       "  'is_sarcastic': 0}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datastore[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yE03fN3NMKpx"
   },
   "source": [
    "**Notes on Tokenization using Keras(Tensor Flow)**:\n",
    "- When dealing with blobs(images) we have pixel values which are numbers(gray scale encoded). But, whilst dealing with text which is usually in raw format, has to be encoded, so that it can be easily preprocessed. we dont have a generic framework to do that, instead we have built in function in keras to do that. encding is a higher side of processing because of higher chunks of data(MB and GB).Whenever we do some relatime application (lstm classification, basically 'coz of complexities of data).\n",
    "- When we start to label for each of the word, to look at the similarity between words it becomes difficult, here the technique of bigrams, trigrams and ngrams. \n",
    "- this is where the keras (tensor flow) comes in which has an easier method.\n",
    "- Using tokenization in keras, we can labels each word and provide a dictionary of the words used in the sentences to the model.\n",
    "- Function used is **fit_on_texts()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "132nOFYVMQ6O"
   },
   "outputs": [],
   "source": [
    "pip install tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vX_OJ4ROPR00"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QQnxgsOZP8Iy",
    "outputId": "103805a3-29ad-4588-e208-28b94f49a296"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rahul', 'is', 'a', 'very', 'good', 'boy']\n"
     ]
    }
   ],
   "source": [
    "#demo\n",
    "sentence = \"Rahul is a very good boy\"\n",
    "res = text_to_word_sequence(sentence)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0F7ocn8VQJag"
   },
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zTAEfnFdQKqM",
    "outputId": "b18bd3f2-7261-45e7-8d5c-2d0af6a396aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('r', 2), ('a', 2), ('h', 1), ('u', 1), ('l', 1), ('i', 1), ('s', 1), ('v', 1), ('e', 1), ('y', 2), ('g', 1), ('o', 3), ('d', 1), ('b', 1)])\n",
      "24\n",
      "{'o': 1, 'r': 2, 'a': 3, 'y': 4, 'h': 5, 'u': 6, 'l': 7, 'i': 8, 's': 9, 'v': 10, 'e': 11, 'g': 12, 'd': 13, 'b': 14}\n",
      "defaultdict(<class 'int'>, {'r': 2, 'a': 2, 'h': 1, 'u': 1, 'l': 1, 'i': 1, 's': 1, 'v': 1, 'e': 1, 'y': 2, 'g': 1, 'o': 3, 'd': 1, 'b': 1})\n"
     ]
    }
   ],
   "source": [
    "print(t.word_counts)\n",
    "print(t.document_count)\n",
    "print(t.word_index)\n",
    "print(t.word_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v8lKnA1xwP6n"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlNTwFJ4Qi3X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3r9gZKKxMTo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Sarcasm Classifier using NLP .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
